# Prompt-Augmented LLM Demo

This repository contains a small demo that shows how to use a public prompt
database to enrich user questions before sending them to a local LLM.  The goal
is to provide additional context and reduce hallucination.  The system is split
into three services:

1. **Backend** – exposes an API that receives a user prompt, queries the prompt database for similar prompts and then calls the LLM. Implemented with FastAPI.
2. **Public context database** – a [Qdrant](https://qdrant.tech/) vector database populated with user prompts from the open dataset [RyokoAI/ShareGPT52K](https://huggingface.co/datasets/RyokoAI/ShareGPT52K).
3. **LLM** – provided by [Ollama](https://github.com/ollama/ollama) (default tag `tinyllama:1.1b`, fits into < 1 GB RAM).

When a prompt arrives the backend retrieves the five most similar prompts from
Qdrant, prepends them as additional context and sends the combined prompt to the
LLM.  For comparison the LLM is also queried with the original user prompt.  The
`client` script displays the retrieved prompts and both answers so you can see
the effect of the augmentation.

## Quick start

1. Install [Docker](https://docs.docker.com/get-docker/) and
   [docker compose](https://docs.docker.com/compose/).
2. Start all services:

```bash
# optional: pull once, then run compose
docker compose exec ollama ollama pull tinyllama:1.1b
docker compose up --build
```

The first start downloads the model and automatically ingests the ShareGPT dataset 
into Qdrant (~52k unique prompts from real conversations). The system will ingest up to 
`PROMPT_LIMIT` prompts (default: 20,000 for fast demo). This takes ~8-10 minutes on CPU. 
If the dataset cannot be downloaded due to network restrictions, the backend falls back 
to local sample prompts so the demo still works offline.

3. In another terminal, send a query:

```bash
python client/client.py "How do I bake bread?"
```

Or test the API directly:

```bash
curl -X POST http://localhost:8000/chat \
     -H "Content-Type: application/json" \
     -d '{"prompt": "Hello, how are you?"}'
```

The script shows the top‑5 similar prompts found in Qdrant and prints both the
"original" answer (using only the user prompt) and the "augmented" answer (using
the merged prompt).

## Development and Troubleshooting

### First-time run (automatic ingestion)

`docker compose up --build` will detect an empty Qdrant collection and automatically download the ShareGPT dataset (~52k prompts), embed, and ingest the first `PROMPT_LIMIT` (default 20k) prompts. This takes ~8-10 minutes on CPU.

To change the limit:

```bash
PROMPT_LIMIT=5000 docker compose up --build
```

Watch the ingestion progress:

```bash
docker compose logs backend -f
```

You should see logs like:
- `"Background ingestion thread started"`
- `"Loading ShareGPT dataset..."`
- `"Ingestion complete. Total prompts ingested: 20000"`

### Expected API Response

After ingestion, a `/chat` call should return semantically relevant prompts:

```json
{
  "original_answer": "Bread baking is a science...",
  "final_answer": "Here are five proven steps to bake soft bread...",
  "similar": [
    "Any tips for baking soft white bread at home?",
    "Why does my homemade bread not rise properly?", 
    "How can I make crusty sourdough with minimal equipment?",
    "What's the ideal oven temperature for baking whole-wheat bread?",
    "How do I store freshly baked bread to keep it crisp?"
  ]
}
```

- `similar` contains 5 **semantically relevant** prompts (no generic/unrelated content)
- Uses advanced BAAI/bge-base-en-v1.5 embeddings (768-dim) for better semantic matching
- Score filtering (≥0.35 cosine similarity) ensures high-quality matches
- Deduplication removes identical or near-identical prompts
- Both `original_answer` and `final_answer` are generated by Ollama
- `final_answer` benefits from contextual prompts and may differ significantly

### Rebuilding after code changes

Always rebuild the backend container after making changes to the code:

```bash
# Stop all services
docker compose down

# Rebuild backend container (forces fresh build)
docker compose build --no-cache backend

# Start services
docker compose up -d
```

Or in one command:

```bash
docker compose up --build
```

### Manual data management

The ingestion is automatic and idempotent. To manually trigger or reset:

```bash
# Check collection status
curl http://localhost:6333/collections/sharegpt-prompts

# Clear collection (will trigger re-ingestion on next start)
curl -X DELETE http://localhost:6333/collections/sharegpt-prompts

# Manual ingestion (if needed)
docker exec demodb-backend-1 python download_and_ingest.py
```

### Tuning Retrieval Quality

The system uses several parameters to ensure high-quality semantic matching:

```bash
# Adjust similarity threshold (0.0-1.0, higher = more strict)
SCORE_THRESHOLD=0.30 docker compose up --build

# Use different embedding model
EMB_MODEL="sentence-transformers/all-MiniLM-L6-v2" docker compose up --build

# Force collection recreation after model changes
FORCE_RECREATE=true docker compose up --build
```

**Default settings**: `BAAI/bge-base-en-v1.5` embeddings with 0.35 similarity threshold provide good balance of relevance and recall.

### Troubleshooting

**No similar prompts returned**: Check that ingestion completed successfully in the logs.

**Too few similar prompts**: Lower `SCORE_THRESHOLD` to 0.30 or 0.25.

**Unrelated prompts returned**: Raise `SCORE_THRESHOLD` to 0.40 or 0.45.

**Ollama timeout errors**: The LLM model may still be downloading. Check with:
```bash
docker compose logs ollama -f
```

**Collection dimension errors**: Set `FORCE_RECREATE=true` when switching embedding models.

## Files

- `backend/` – FastAPI app and ingestion script.
- `client/` – simple CLI client.
- `docker-compose.yml` – launches Qdrant, Ollama and the backend service.


