# Prompt-Augmented LLM Demo

This demo shows how to combine a local LLM with a public prompt database to reduce hallucination. It consists of three parts:

1. **Backend** – exposes an API that receives a user prompt, queries the prompt database for similar prompts and then calls the LLM. Implemented with FastAPI.
2. **Public context database** – a [Qdrant](https://qdrant.tech/) vector database populated with user prompts from the open dataset [RyokoAI/ShareGPT52K](https://huggingface.co/datasets/RyokoAI/ShareGPT52K).
3. **LLM** – provided by [Ollama](https://github.com/ollama/ollama) running the `llama3:8b-instruct` model.

The `client` script talks to the backend and prints both the retrieved prompts and the final answer.

## Quick start

1. Install [Docker](https://docs.docker.com/get-docker/) and [docker compose](https://docs.docker.com/compose/).
2. Run the stack:

```bash
docker compose up
```

The first start downloads the model and ingests the ShareGPT dataset into Qdrant (about 20k prompts for a fast demo).

3. In another terminal, query the system:

```bash
python client/client.py "How do I bake bread?"
```

The script prints the top‑5 most similar prompts retrieved from Qdrant and the answer generated by the LLM.

## Files

- `backend/` – FastAPI app and ingestion script.
- `client/` – simple CLI client.
- `docker-compose.yml` – launches Qdrant, Ollama and the backend service.

